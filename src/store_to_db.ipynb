{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e284703a",
   "metadata": {},
   "source": [
    "# Split text from markdown file and store chunks to PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"path_to_the_file\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f096bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def split_text_into_chunks(text: str, max_tokens: int = 250) -> List[str]:\n",
    "    paragraphs = re.split(r'(?:\\n\\s*\\n|</p>\\s*<p>)', text.strip())\n",
    "    chunks, current = [], []\n",
    "\n",
    "    def token_len(s):\n",
    "        return len(s.split())\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "        if token_len(\" \".join(current) + \" \" + para) > max_tokens:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "                current = []\n",
    "        current.append(para)\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current).strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def preprocess_pdf_text(document_text: str, max_tokens: int = 250) -> List[Dict]:\n",
    "    pages = re.split(r'\\n\\s*---\\s*\\n', document_text.strip())\n",
    "    all_chunks = []\n",
    "\n",
    "    for page_number, page_text in enumerate(pages, start=1):\n",
    "        pos = 0\n",
    "        for match in re.finditer(r'<table.*?>.*?</table>', page_text, re.DOTALL | re.IGNORECASE):\n",
    "            start, end = match.span()\n",
    "\n",
    "            # text before the table -> normal chunks\n",
    "            before = page_text[pos:start].strip()\n",
    "            if before:\n",
    "                for chunk in split_text_into_chunks(before, max_tokens=max_tokens):\n",
    "                    all_chunks.append({\n",
    "                        \"page_number\": page_number,\n",
    "                        \"content\": chunk\n",
    "                    })\n",
    "\n",
    "            # table block -> keep as is\n",
    "            table_block = match.group(0).strip()\n",
    "            all_chunks.append({\n",
    "                \"page_number\": page_number,\n",
    "                \"content\": table_block\n",
    "            })\n",
    "\n",
    "            pos = end\n",
    "\n",
    "        # text after the last table\n",
    "        after = page_text[pos:].strip()\n",
    "        if after:\n",
    "            for chunk in split_text_into_chunks(after, max_tokens=max_tokens):\n",
    "                all_chunks.append({\n",
    "                    \"page_number\": page_number,\n",
    "                    \"content\": chunk\n",
    "                })\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = preprocess_pdf_text(text, max_tokens=1000)\n",
    "\n",
    "for c in chunks:\n",
    "    print(f\"Page {c['page_number']}\\n{c['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf082cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(chunks)\n",
    "\n",
    "df[\"filename\"] = \"filename\"\n",
    "df = df[[\"filename\", \"page_number\", \"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True, device=device)\n",
    "\n",
    "embeddings = []\n",
    "batch_size = 12\n",
    "\n",
    "texts = df[\"content\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, batch_size=len(batch), max_length=8192)[\"dense_vecs\"]\n",
    "    embeddings.extend(batch_embeddings.tolist())\n",
    "\n",
    "df[\"embedding\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": int(os.getenv(\"DB_PORT\", 5432))\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c18682",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS chunks_table (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        \"filename\" TEXT,\n",
    "        \"page_number\" SMALLINT,\n",
    "        \"content\" TEXT,\n",
    "        \"embedding\" VECTOR(1024)\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table is created or already exists\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_batch\n",
    "import ast\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO chunks_table (\n",
    "    \"filename\", \"page_number\", \"content\",\n",
    "    \"embedding\"\n",
    ") VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "records = [\n",
    "    (\n",
    "        row[\"filename\"],\n",
    "        row[\"page_number\"],\n",
    "        row[\"content\"],\n",
    "        row[\"embedding\"]\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "execute_batch(cursor, insert_query, records)\n",
    "conn.commit()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833c951",
   "metadata": {},
   "source": [
    "# Store structured files to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "input_path = \"path_to_input_folder\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fname in os.listdir(input_path):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    base_match = re.match(r\"(.+)\\.pdf_page(\\d+)\\.txt\", fname)\n",
    "    if not base_match:\n",
    "        continue\n",
    "\n",
    "    pdf_name = base_match.group(1) + \".pdf\"\n",
    "    page_number = int(base_match.group(2))\n",
    "\n",
    "    with open(os.path.join(input_path, fname), encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    parts = re.split(r'(?=id:\\s*\\d+)', text.strip())\n",
    "\n",
    "    if parts and not parts[0].strip().startswith(\"id:\"):\n",
    "        header = parts[0].strip()\n",
    "        if header:\n",
    "            rows.append({\n",
    "                \"filename\": pdf_name,\n",
    "                \"page_number\": page_number,\n",
    "                \"content\": header\n",
    "            })\n",
    "        parts = parts[1:]\n",
    "\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            rows.append({\n",
    "                \"filename\": pdf_name,\n",
    "                \"page_number\": page_number,\n",
    "                \"content\": part.strip()\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True, device=device)\n",
    "\n",
    "embeddings = []\n",
    "batch_size = 12\n",
    "\n",
    "texts = df[\"content\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, batch_size=len(batch), max_length=8192)[\"dense_vecs\"]\n",
    "    embeddings.extend(batch_embeddings.tolist())\n",
    "\n",
    "df[\"embedding\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": int(os.getenv(\"DB_PORT\", 5432))\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS chunks_table (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        \"filename\" TEXT,\n",
    "        \"page_number\" SMALLINT,\n",
    "        \"content\" TEXT,\n",
    "        \"embedding\" VECTOR(1024)\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table created or already exists\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_batch\n",
    "import ast\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO chunks_table (\n",
    "    \"filename\", \"page_number\", \"content\",\n",
    "    \"embedding\"\n",
    ") VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "records = [\n",
    "    (\n",
    "        row[\"filename\"],\n",
    "        row[\"page_number\"],\n",
    "        row[\"content\"],\n",
    "        row[\"embedding\"]\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "execute_batch(cursor, insert_query, records)\n",
    "conn.commit()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
